import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs
import json
import os
import shutil
import img2pdf
import time

# ================= é…ç½®åŒº =================
USERNAME = "ä½ çš„è´¦å·"
PASSWORD = "ä½ çš„å¯†ç "
TARGET_COURSE_URL = "https://classroom.msa.buaa.edu.cn/livingroom?course_id=115538&sub_id=5377422&tenant_code=21"
# ==========================================

# è¿™é‡Œçš„ service å‚æ•°å¿…é¡»å’Œæµè§ˆå™¨é‡Œçš„ä¸€æ¨¡ä¸€æ ·ï¼Œå¦åˆ™ CAS ä¼šæŠ¥é”™
LOGIN_URL = "https://sso.buaa.edu.cn/login?service=https%3A%2F%2Fyjapi.msa.buaa.edu.cn%2Fcasapi%2Findex.php%3Fforward%3Dhttps%253A%252F%252Fclassroom.msa.buaa.edu.cn%252F%26r%3Dauth%252Flogin%26tenant_code%3D21"

session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
})

def auto_login():
    print("ğŸš€ [1/4] æ­£åœ¨è·å–ç™»å½•å‚æ•°...")
    try:
        resp = session.get(LOGIN_URL)
        soup = BeautifulSoup(resp.text, 'html.parser')
        execution = soup.find('input', {'name': 'execution'})['value']
        event_id = soup.find('input', {'name': '_eventId'})['value']
    except Exception as e:
        print(f"   âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

    payload = {
        "username": USERNAME,
        "password": PASSWORD,
        "execution": execution,
        "_eventId": event_id,
        "submit": "ç™»å½•",
        "type": "username_password"
    }
    
    print("ğŸš€ [2/4] å‘é€è´¦å·å¯†ç ...")
    # è¿™ä¸€æ­¥è¯·æ±‚ ssoï¼ŒæˆåŠŸåä¼šè¿”å› 302 è·³è½¬ï¼Œrequests é»˜è®¤ä¼šè‡ªåŠ¨è·Ÿéšè·³è½¬
    # æˆ‘ä»¬éœ€è¦å®ƒè‡ªåŠ¨è·Ÿéšï¼Œå› ä¸ºå®ƒä¼šè·³è½¬åˆ° classroom.../login?ticket=...
    # å¹¶åœ¨é‚£ä¸ªé¡µé¢è¢«æœåŠ¡å™¨ç§ä¸‹ Cookie
    login_resp = session.post(LOGIN_URL, data=payload)
    
    # æ£€æŸ¥æ˜¯å¦åˆ°äº† classroom åŸŸå
    current_url = login_resp.url
    print(f"   å½“å‰ URL: {current_url}")
    
    # æ£€æŸ¥ Cookie æ˜¯å¦å­˜åœ¨
    # æˆ‘ä»¬é‡ç‚¹æ‰¾ '_token' è¿™ä¸ª cookie
    cookies = session.cookies.get_dict()
    if '_token' in cookies:
        print("   âœ… ç™»å½•æˆåŠŸï¼å·²è·å– _token Cookieã€‚")
        # æ‰“å°ä¸€ä¸‹çœ‹çœ‹æ˜¯ä¸æ˜¯å’Œæµè§ˆå™¨é‡Œä¸€æ ·é•¿
        print(f"   Cookie é•¿åº¦: {len(cookies['_token'])}")
        return True
    else:
        # å¦‚æœ URL é‡Œæœ‰ ticket ä½†æ²¡æœ‰ cookieï¼Œå¯èƒ½è„šæœ¬æ²¡è‡ªåŠ¨è·³è½¬ï¼Œæ‰‹åŠ¨è®¿é—®ä¸€ä¸‹
        if "ticket=" in current_url:
            print("   âš ï¸ å‘ç° Ticket ä½†æœªè·å– Cookieï¼Œå°è¯•æ‰‹åŠ¨æ¿€æ´»...")
            session.get(current_url)
            if '_token' in session.cookies.get_dict():
                print("   âœ… è¡¥æ•‘æˆåŠŸï¼Cookie å·²è·å–ã€‚")
                return True
        
        print("   âŒ ç™»å½•å¤±è´¥ï¼šæœªè·å–åˆ°æœ‰æ•ˆ Cookieã€‚")
        print("   è°ƒè¯•ä¿¡æ¯ - å½“å‰ Cookie:", cookies)
        return False

def get_course_info(url):
    print("ğŸš€ [3/4] è§£æè¯¾ç¨‹ä¿¡æ¯...")
    parsed = urlparse(url)
    params = parse_qs(parsed.query)
    c_id = params.get('course_id', [None])[0]
    s_id = params.get('sub_id', [None])[0]
    
    api_url = f"https://classroom.msa.buaa.edu.cn/courseapi/v3/portal-home-setting/get-sub-info?course_id={c_id}&sub_id={s_id}"
    
    # æ­¤æ—¶è¯·æ±‚ä¼šè‡ªåŠ¨å¸¦ä¸Š Cookie
    resp = session.get(api_url)
    
    try:
        data = resp.json()
    except:
        print(f"   âŒ API å“åº”é JSONï¼Œå¯èƒ½ Cookie æ— æ•ˆã€‚")
        return None, None, None

    if data.get('code') != 0:
        print(f"   âŒ API æŠ¥é”™: {data.get('msg')}")
        return None, None, None
        
    info = data['data']
    guid = info.get('resource_guid')
    title = info.get('sub_title', 'PPT_Export').strip()
    title = "".join([c for c in title if c.isalnum() or c in (' ', '-', '_')])
    
    return c_id, s_id, guid, title

def download_ppt(c_id, s_id, guid, title):
    print("ğŸš€ [4/4] ä¸‹è½½ PPT...")
    api_url = f"https://classroom.msa.buaa.edu.cn/pptnote/v1/schedule/search-ppt?course_id={c_id}&sub_id={s_id}&page=1&per_page=500&resource_guid={guid}"
    
    resp = session.get(api_url)
    data = resp.json()
    
    slide_urls = []
    if data.get('list'):
        for item in data['list']:
            try:
                c = json.loads(item['content'])
                if c.get('pptimgurl'): slide_urls.append(c['pptimgurl'])
            except: continue

    if not slide_urls:
        print("   âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡")
        return

    temp_dir = "temp_slides_final"
    if os.path.exists(temp_dir): shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)
    
    files = []
    print(f"   ğŸ“„ å¼€å§‹ä¸‹è½½ {len(slide_urls)} å¼ å›¾ç‰‡...")
    for i, url in enumerate(slide_urls):
        fname = os.path.join(temp_dir, f"{i:03d}.jpg")
        r = requests.get(url) # å›¾ç‰‡ä¸€èˆ¬ä¸éœ€è¦ Cookieï¼Œä½†å¸¦ä¸Šä¹Ÿæ— å¦¨
        with open(fname, 'wb') as f: f.write(r.content)
        files.append(fname)
        print(f"\r   â¬‡ï¸ {i+1}/{len(slide_urls)}", end="")
        
    print(f"\n   ğŸ“¦ åˆæˆ PDF: {title}.pdf")
    with open(f"{title}.pdf", "wb") as f:
        f.write(img2pdf.convert(files))
    shutil.rmtree(temp_dir)
    print("âœ… å®Œæˆï¼")

if __name__ == "__main__":
    if auto_login():
        res = get_course_info(TARGET_COURSE_URL)
        if res and res[0]:
            download_ppt(*res)